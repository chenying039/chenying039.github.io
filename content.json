{"meta":{"title":"Hexo","subtitle":"","description":"blog","author":"Tisi is chenying039","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"","slug":"My-New-Post","date":"2022-05-31T13:32:13.101Z","updated":"2022-05-31T13:34:51.478Z","comments":true,"path":"2022/05/31/My-New-Post/","link":"","permalink":"http://example.com/2022/05/31/My-New-Post/","excerpt":"","text":"title: My New Postdate: 2022-05-31 21:32:13tags:《Spark HA&amp;Yarn配置》Spark（HA）支持的Zookeeper版本为zookeeper-3.7.0版本（1）修改spark-env.sh配置文件删除SPARK_MASTER_HOST&#x3D;node1不固定master节点增加内容：（2）将spark-env.sh分发到node2和node3scp spark-env.sh node2:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;scp spark-env.sh node3:&#x2F;export&#x2F;server&#x2F;spark&#x2F;conf&#x2F;关闭当前StandAlone集群(之前需开启配置完成的zookeeper)（4）启动集群:在node1上 启动一个master 和全部workercd &#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-all.sh在node2上启动一个备用的master进程（5）查看状态：①node1:8080端口状态为ALIVE②在node2上是备用master，当node1启用master时node2状态为standby(8080端口可能会发生顺延)如下端口顺延为8082：关闭node1的master进程，node2master启用其状态切换为ALIVE（6）测试：提交一个spark任务到当前ALIVEmaster上:bin&#x2F;spark-submit –master spark:&#x2F;&#x2F;node1:7077 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 1000提交后，将ALIVEmaster kill掉，不会影响程序当新的master（即使用node2备用master）接收集群后, 程序继续运行, 正常得到结果5.：HA模式下，主备切换不会影响正在运行的程序Spark(Yarn)1.保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中3.链接到YARN中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）4.client 模式测试运行&#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode cluster –driver-memory 512m –executor-memory 512m –num-executors 3 –total-executor-cores 3 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 3 测试yarn提交spark任务5.cluster 模式测试&#x2F;export&#x2F;server&#x2F;spark&#x2F;bin&#x2F;spark-submit –master yarn –deploy-mode cluster –driver-memory 512m –executor-memory 512m –num-executors 3 –total-executor-cores 3 &#x2F;export&#x2F;server&#x2F;spark&#x2F;examples&#x2F;src&#x2F;main&#x2F;python&#x2F;pi.py 3通过web UI查看任务运行状态http://master:8080/spark-submit任务提交后，由yarn负责资源调度http://master:19888/spark:部署模式Spark 有多种运行模式， Spark 支持本地运行模式（Local 模式）、独立运行模式（Standalone 模式）、YARN（Yet Another Resource Negotiator）local(本地模式)：常用于本地开发测试，本地还分为local单线程和local-cluster多线程;standalone(集群模式)：典型的Mater&#x2F;slave模式，不过也能看出Master是有单点故障的；Spark支持ZooKeeper来实现 HAon yarn(集群模式)： 运行在 yarn 资源管理器框架之上，由 yarn 负责资源管理，Spark 负责任务调度和计算Spark运行架构包括：Master（集群资源管理）、Slaves（运行任务的工作节点）、应用程序的控制节点（Driver）和每个工作节点上负责任务的执行进程（Executor）；2、Master是集群资源的管理者（Cluster Manager）。支持：Standalone,Yarn,Mesos；Slaves在spark中被称为Worker，工作节点，；1.Spark的计算模式属于MapReduce,在借鉴Hadoop MapReduce优点的同时很好地解决了MapReduce所面临的问题2.不局限于Map和Reduce操作，还提供了多种数据集操作类型，编程模型比Hadoop MapReduce更灵活3.Spark提供了内存计算，可将中间结果放到内存中，对于迭代运算效率更高4.Spark基于DAG的任务调度执行机制，要优于Hadoop MapReduce的迭代执行机制(函数调用)spark on yarn 的支持两种模式： yarn-cluster：适用于生产环境 yarn-client：适用于交互、调试，希望立即看到app的输出standalone模式，即独立模式，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统。从一定程度上说，该模式是其他两种的基础。","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2022-05-31T11:06:11.987Z","updated":"2022-05-31T11:06:11.987Z","comments":true,"path":"2022/05/31/hello-world/","link":"","permalink":"http://example.com/2022/05/31/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}